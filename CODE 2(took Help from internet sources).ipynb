{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 57) (921, 57)\n",
      "Train Loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakhar Saxena\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Prakhar Saxena\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:30: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = nan\n",
      "Train Loss = nan\n",
      "Train Loss = 0.619011922473\n",
      "Train Loss = 0.609412647066\n",
      "Train Loss = 0.601081108911\n",
      "Train Loss = 0.594022518357\n",
      "Train Loss = 0.588468429889\n",
      "Train Loss = 0.583210006798\n",
      "Train Loss = 0.578102496577\n",
      "76.0043431053203\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "class logistic_classifier(object):\n",
    "    def __init__(self, coeff):\n",
    "        # coeff is the regularization coefficient\n",
    "        self.coeff = coeff\n",
    "        # w will be our linear separator\n",
    "        self.w = 0.0\n",
    "\n",
    "\n",
    "    def compute_probabilities(self, Xtr):\n",
    "        # function to compute mu_i (sigmoid(wTx)).\n",
    "        # Complete this function. \n",
    "        #print(\"sigmoid\")   \n",
    "        z = np.dot(Xtr, self.w)\n",
    "        \n",
    "        h = self.sigmoid(z)\n",
    "        #print (h)\n",
    "        return h\n",
    "        #pass\n",
    "        \n",
    "    def compute_loss(self, probabilities, Ytr):\n",
    "        # function to compute training loss.\n",
    "        # Keep in mind that log(x) tends to infinity if x is too close\n",
    "        # to zero. Thus, whenever you think x could be close to\n",
    "        # zero, add a small positive value (like 10^(-6)) to x so that\n",
    "        # log doesn't return too big a value. \n",
    "        \n",
    "        mm = (-Ytr * np.log(probabilities) - (1 - Ytr) * np.log(1 - probabilities)).mean()\n",
    "        #print (\"loss =\" + mm) \n",
    "       \n",
    "        return mm\n",
    "        #pass\n",
    "        \n",
    "    def compute_gradients(self, probabilities, Ytr, Xtr):\n",
    "        # function to compute gradients with respect to w.\n",
    "        # Complete it, keeping in mind the two gradient components -\n",
    "        # the one for the logistic loss and the one for the regularizer.\n",
    "        grad = np.dot(Xtr.T, (probabilities - Ytr))\n",
    "        #print (\"grad = \" , grad )\n",
    "        return grad\n",
    "        #pass\n",
    "\n",
    "    def update_weights(self, learning_rate, grads):\n",
    "        # function to update weights. This needs to be filled in. \n",
    "        self.w -= learning_rate * grads\n",
    "        pass\n",
    "\n",
    "    def sigmoid(self, inputs):\n",
    "        # function to compute sigmoid of the inputs.\n",
    "        # Keep in mind that if you exponentiate too large or too small\n",
    "        # a value, the result might be out of bounds of computer\n",
    "        # precision. Thus, put a lower and an upper cap on the\n",
    "        # input to the exponential function.\n",
    "        #print (input)\n",
    "        ghj = np.exp(-inputs)\n",
    "        np.clip(ghj,-999999,999999,out=ghj)\n",
    "        return 1 / (1 + ghj) \n",
    "        #pass\n",
    "        \n",
    "    def fit(self, Xtr, Ytr):\n",
    "        '''\n",
    "        This function trains the logistic regression model on the \n",
    "        given training data\n",
    "        '''\n",
    "        # num_iters: number of iterations that gradient descent\n",
    "        # should run for.\n",
    "        learning_rate = 0.00005\n",
    "        \n",
    "        self.num_iters = 10000\n",
    "\n",
    "        # random initialization for w. \n",
    "        self.w = np.random.normal(0.0, 0.1, Xtr.shape[1])\n",
    "        \n",
    "        for iter in range(self.num_iters):\n",
    "            probabilities = self.compute_probabilities(Xtr)\n",
    "        \n",
    "            train_loss = self.compute_loss(probabilities, Ytr)\n",
    "            grads = self.compute_gradients(probabilities, Ytr, Xtr)\n",
    "            grads = grads / Xtr.shape[0]\n",
    "            self.update_weights(learning_rate, grads)\n",
    "            if iter % 1000 == 0:\n",
    "                print (\"Train Loss = \" + str(train_loss))\n",
    "            \n",
    "            \n",
    "\n",
    "    def predict(self, Xts):\n",
    "        '''\n",
    "        This function gives label predictions on the dataset fed to it. \n",
    "        '''\n",
    "        linear_combinations = np.matmul(Xts, self.w)\n",
    "        probabilities = self.sigmoid(linear_combinations)\n",
    "        self.predictions = np.zeros(probabilities.shape)\n",
    "        self.predictions = (probabilities > 0.5)\n",
    "        return self.predictions\n",
    "\n",
    "\n",
    "dataset = \"spam\"\n",
    "# adjust the path according to where you have stored the dataset. \n",
    "\n",
    "\n",
    "Xtr = np.load(\"Xtrain.npy\")\n",
    "Ytr = np.load(\"Ytrain.npy\")\n",
    "Xts = np.load(\"Xtest.npy\")\n",
    "Yts = np.load(\"Ytest.npy\")\n",
    "\n",
    "print (Xtr.shape, Xts.shape)\n",
    "\n",
    "model = logistic_classifier(coeff=0.0)\n",
    "model.fit(Xtr, Ytr)\n",
    "predictions = model.predict(Xts)\n",
    "\n",
    "accuracy = 0.0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == Yts[i]:\n",
    "        accuracy += 1\n",
    "accuracy /= len(predictions)\n",
    "accuracy *= 100\n",
    "test_accuracy = accuracy\n",
    "print (test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 57) (921, 57)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "# adjust the path according to where you have stored the dataset. \n",
    "\n",
    "\n",
    "Xtr = np.load( \"Xtrain.npy\")\n",
    "Ytr = np.load( \"Ytrain.npy\")\n",
    "Xts = np.load( \"Xtest.npy\")\n",
    "Yts = np.load( \"Ytest.npy\")\n",
    "\n",
    "print (Xtr.shape, Xts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
