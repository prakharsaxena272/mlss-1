{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 57) (921, 57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakhar Saxena\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:24: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3680,) and (921,) not aligned: 3680 (dim 0) != 921 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-39c9c82b5d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoeff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-39c9c82b5d17>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Xtr, Ytr)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m \u001b[1;34m\"Train Loss = \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-39c9c82b5d17>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, probabilities, Ytr)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[1;31m# zero, add a small positive value (like 10^(-6)) to x so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[1;31m# log doesn't return too big a value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mCost\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mYtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mCost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3680,) and (921,) not aligned: 3680 (dim 0) != 921 (dim 0)"
     ]
    }
   ],
   "source": [
    "class logistic_classifier(object):\n",
    "    def __init__(self, coeff):\n",
    "        # coeff is the regularization coefficient\n",
    "        self.coeff = coeff\n",
    "        # w will be our linear separator\n",
    "        self.w = 0.0\n",
    "\n",
    "\n",
    "    def compute_probabilities(self, Xtr):\n",
    "        # function to compute mu_i (sigmoid(wTx)).\n",
    "        # Complete this function. \n",
    "        linear_combinations = np.matmul(Xts, self.w)\n",
    "        probabilities = self.sigmoid(linear_combinations)\n",
    "        self.predictions = np.zeros(probabilities.shape)\n",
    "        self.predictions = (probabilities > 0.5)\n",
    "        return self.predictions\n",
    "        \n",
    "    def compute_loss(self, probabilities, Ytr):\n",
    "        # function to compute training loss.\n",
    "        # Keep in mind that log(x) tends to infinity if x is too close\n",
    "        # to zero. Thus, whenever you think x could be close to\n",
    "        # zero, add a small positive value (like 10^(-6)) to x so that\n",
    "        # log doesn't return too big a value. \n",
    "        Cost= (np.matmul((Ytr),np.transpose(np.log(probabilities))) + np.matmul((1-Ytr),np.transpose(np.log(1-probabilities)))*(-1/len(probabilities)))\n",
    "        return Cost\n",
    "        \n",
    "    def compute_gradients(self, probabilities, Ytr, Xtr):\n",
    "        # function to compute gradients with respect to w.\n",
    "        # Complete it, keeping in mind the two gradient components -\n",
    "        # the one for the logistic loss and the one for the regularizer.\n",
    "        gradient=np.matmul((probabilities-Ytr), Xtr)\n",
    "        return gradient\n",
    "\n",
    "    def update_weights(self, learning_rate, grads):\n",
    "        # function to update weights. This needs to be filled in. \n",
    "        self.w-=learning_rate * grads\n",
    "        \n",
    "\n",
    "    def sigmoid(self, inputs):\n",
    "        # function to compute sigmoid of the inputs.\n",
    "        # Keep in mind that if you exponentiate too large or too small\n",
    "        # a value, the result might be out of bounds of computer\n",
    "        # precision. Thus, put a lower and an upper cap on the\n",
    "        # input to the exponential function. \n",
    "        IP=-inputs\n",
    "        exp=1/(1 + np.exp(IP))\n",
    "        exp[exp>0.99]=0.99\n",
    "        exp[exp>0.01]=0.01    \n",
    "        return exp\n",
    "        \n",
    "    def fit(self, Xtr, Ytr):\n",
    "        '''\n",
    "        This function trains the logistic regression model on the \n",
    "        given training data\n",
    "        '''\n",
    "        # num_iters: number of iterations that gradient descent\n",
    "        # should run for.\n",
    "        learning_rate = 0.00005\n",
    "        \n",
    "        self.num_iters = 10000\n",
    "\n",
    "        # random initialization for w. \n",
    "        self.w = np.random.normal(0.0, 0.1, Xtr.shape[1])\n",
    "        \n",
    "        for iter in range(self.num_iters):\n",
    "            probabilities = self.compute_probabilities(Xtr)\n",
    "        \n",
    "            train_loss = self.compute_loss(probabilities, Ytr)\n",
    "            if iter % 1000 == 0:\n",
    "                print ( \"Train Loss = \"+str(train_loss))\n",
    "            \n",
    "            grads = self.compute_gradients(probabilities, Ytr, Xtr)\n",
    "            grads = grads / Xtr.shape[0]\n",
    "            self.update_weights(learning_rate, grads)\n",
    "\n",
    "    def predict(self, Xts):\n",
    "        '''\n",
    "        This function gives label predictions on the dataset fed to it. \n",
    "        '''\n",
    "        linear_combinations = np.matmul(Xts, self.w)\n",
    "        probabilities = self.sigmoid(linear_combinations)\n",
    "        self.predictions = np.zeros(probabilities.shape)\n",
    "        self.predictions = (probabilities > 0.5)\n",
    "        return self.predictions\n",
    "\n",
    "dataset = \"spam\"\n",
    "\n",
    "\n",
    "Xtr = np.load( \"Xtrain.npy\")\n",
    "Ytr = np.load(\"Ytrain.npy\")\n",
    "Xts = np.load(\"Xtest.npy\")\n",
    "Yts = np.load(\"Ytest.npy\")\n",
    "\n",
    "print (Xtr.shape, Xts.shape)\n",
    "\n",
    "model = logistic_classifier(coeff=0.0)\n",
    "model.fit(Xtr, Ytr)\n",
    "predictions = model.predict(Xts)\n",
    "\n",
    "accuracy = 0.0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == Yts[i]:\n",
    "        accuracy += 1\n",
    "accuracy /= len(predictions)\n",
    "accuracy *= 100\n",
    "test_accuracy = accuracy\n",
    "print (test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 57) (921, 57)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logistic_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4866a4a8e437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoeff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logistic_classifier' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
